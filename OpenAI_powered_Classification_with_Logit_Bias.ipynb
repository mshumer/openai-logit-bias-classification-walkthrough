{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeH5z57O5pL8hydPkIaZgz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshumer/openai-logit-bias-classification-walkthrough/blob/main/OpenAI_powered_Classification_with_Logit_Bias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logit Bias for OpenAI-powered Classification\n",
        "By Matt Shumer (https://twitter.com/mattshumer_)\n",
        "\n",
        "Github repo: https://github.com/mshumer/openai-logit-bias-classification-walkthrough\n",
        "\n",
        "Reusable notebook for creating classifiers with OpenAI models and logit bias, benchmarking each model's performance for your use-case, and exporting the classifiers you create for use in your code.\n",
        "\n",
        "In the first cell, add in your OpenAI key to get started."
      ],
      "metadata": {
        "id": "0r5J_dOVbgV0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXRSIq7hkyt9"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install prettytable\n",
        "!pip install tiktoken\n",
        "\n",
        "import openai\n",
        "import tiktoken\n",
        "import prettytable\n",
        "\n",
        "openai.api_key = \"ADD YOUR OPENAI KEY HERE\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First, a bit about logit bias.**\n",
        "\n",
        "> Logit bias is a technique used to influence the output probabilities of a machine learning model, specifically in the context of natural language processing models like GPT-3. It works by adding a constant value to the logits (pre-activation scores) of specific tokens (words or phrases) before they are passed through the activation function to calculate probabilities.\n",
        "\n",
        "In simpler terms, logit bias helps you guide the model's output towards certain tokens, making them more or less likely to be chosen in the generated text. It's a way to control the model's behavior by nudging it towards the desired output.\n",
        "\n",
        "Now, let's get started. In this notebook, we're using logit bias to create powerful classifiers with very little effort."
      ],
      "metadata": {
        "id": "EjGSkXZCdV0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------"
      ],
      "metadata": {
        "id": "HjU1TYuXdyzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we're creating a binary true/false classifier. We're using it to classify statements as true or false, but feel free to modify it to support your use-case, by a) updating the test cases, and b) updating the messages in the OpenAI call.\n",
        "\n",
        "\n",
        "This cell is used to benchmark multiple OpenAI models against your use-case, to show you which model is best. You can also see the average latency of each model.\n",
        "\n",
        "\n",
        "As with all the cells in this notebook, you can adjust the models if you don't yet have access to GPT-4."
      ],
      "metadata": {
        "id": "09OC8qmUcPH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "import time\n",
        "\n",
        "# here we're doing true or false statements... feel free to adjust for your use-case\n",
        "test_cases = [\n",
        "    {\n",
        "        'input': 'The United States includes New York City.',\n",
        "        'expected_output': 'true'\n",
        "    },\n",
        "    {\n",
        "        'input': 'The Earth is flat.',\n",
        "        'expected_output': 'false'\n",
        "    },\n",
        "    {\n",
        "        'input': 'The capital of France is Paris.',\n",
        "        'expected_output': 'true'\n",
        "    },\n",
        "    {\n",
        "        'input': 'The Moon is made of green cheese.',\n",
        "        'expected_output': 'false'\n",
        "    },\n",
        "    {\n",
        "        'input': 'Water boils at 100 degrees Celsius at sea level.',\n",
        "        'expected_output': 'true'\n",
        "    }\n",
        "]\n",
        "\n",
        "models = ['gpt-4', 'gpt-4-0613', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo'] # choose what models you'd like to benchmark\n",
        "model_results = {model: {'correct': 0, 'total': 0} for model in models}\n",
        "\n",
        "# Initialize the table\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Input\", \"Expected\"] + models\n",
        "\n",
        "# Wrap the text in the \"Input\" column\n",
        "table.max_width[\"Input\"] = 100\n",
        "\n",
        "# Initialize timers\n",
        "model_timers = {model: 0 for model in models}\n",
        "\n",
        "for test_case in test_cases:\n",
        "    row = [test_case['input'], test_case['expected_output']]\n",
        "    for model in models:\n",
        "        start_time = time.time()\n",
        "\n",
        "        x = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Determine if the input statement is true or false. Return 'true' if it is true, return 'false' if it is false.\"}, # adjust for your use-case\n",
        "                {\"role\": \"user\", \"content\": f\"Here is the statement: `{test_case['input']}`\"} # adjust for your use-case\n",
        "            ],\n",
        "            logit_bias={\n",
        "                '1904': 100,  # true to 100\n",
        "                '3934': 100,  # false to 100\n",
        "            },\n",
        "            max_tokens=1,\n",
        "            temperature=0,\n",
        "        ).choices[0].message.content\n",
        "\n",
        "        end_time = time.time()\n",
        "        model_timers[model] += (end_time - start_time)\n",
        "\n",
        "        status = \"✅\" if x == test_case['expected_output'] else \"❌\"\n",
        "        row.append(status)\n",
        "\n",
        "        # Update model results\n",
        "        if x == test_case['expected_output']:\n",
        "            model_results[model]['correct'] += 1\n",
        "        model_results[model]['total'] += 1\n",
        "\n",
        "    table.add_row(row)\n",
        "\n",
        "print(table)\n",
        "\n",
        "# Calculate and print the percentage of correct answers and average time for each model\n",
        "for model in models:\n",
        "    correct = model_results[model]['correct']\n",
        "    total = model_results[model]['total']\n",
        "    percentage = (correct / total) * 100\n",
        "    avg_time = model_timers[model] / total\n",
        "    print(f\"{model} got {percentage:.2f}% correct. Average time: {avg_time:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "iCr3z66jvPhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we're creating a more flexible classifier, with inputs we can define. In the example here, we're using it to sort statements into happy, sad, neutral buckets, but feel free to modify it to support your use-case, by a) updating the test cases, and b) updating the messages in the OpenAI call.\n",
        "\n",
        "\n",
        "This cell is used to benchmark multiple OpenAI models against your use-case, to show you which model is best. You can also see the average latency of each model.\n",
        "\n",
        "\n",
        "As with all the cells in this notebook, you can adjust the models if you don't yet have access to GPT-4."
      ],
      "metadata": {
        "id": "iVf_YWZec8os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "import time\n",
        "import openai\n",
        "\n",
        "# Function to get token IDs for a given model family\n",
        "def get_token_ids(model_family, text):\n",
        "    if 'gpt-4' in model_family:\n",
        "        model_enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "    else:\n",
        "        model_enc = tiktoken.encoding_for_model(\"gpt-3\")\n",
        "    return model_enc.encode(text)\n",
        "\n",
        "# here we're doing happy, sad, neutral sentiment analysis... feel free to adjust for your use-case\n",
        "test_cases = [\n",
        "    {\n",
        "        'input': 'My name is Matt',\n",
        "        'expected_output': 'neutral'\n",
        "    },\n",
        "    {\n",
        "        'input': 'I am so happy today!',\n",
        "        'expected_output': 'happy'\n",
        "    },\n",
        "    {\n",
        "        'input': 'I had a bad day.',\n",
        "        'expected_output': 'sad'\n",
        "    },\n",
        "    {\n",
        "        'input': 'The temperature is 50 degrees.',\n",
        "        'expected_output': 'neutral'\n",
        "    },\n",
        "    {\n",
        "        'input': 'I just won the lottery!',\n",
        "        'expected_output': 'happy'\n",
        "    }\n",
        "]\n",
        "\n",
        "models = ['gpt-4', 'gpt-4-0613', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo'] # choose what models you'd like to benchmark\n",
        "model_results = {model: {'correct': 0, 'total': 0} for model in models}\n",
        "\n",
        "# Initialize the table\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Input\", \"Expected\"] + models\n",
        "\n",
        "# Wrap the text in the \"Input\" column\n",
        "table.max_width[\"Input\"] = 100\n",
        "\n",
        "# Initialize timers\n",
        "model_timers = {model: 0 for model in models}\n",
        "\n",
        "for test_case in test_cases:\n",
        "\n",
        "    row = [test_case['input'], test_case['expected_output']]\n",
        "\n",
        "    for model in models:\n",
        "\n",
        "        logit_bias_values = {\n",
        "            'happy': get_token_ids(models[0], 'happy')[0],\n",
        "            'sad': get_token_ids(models[0], 'sad')[0],\n",
        "            'neutral': get_token_ids(models[0], 'neutral')[0]\n",
        "        }\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        x = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": \"Determine the sentiment of the input statement. Return 'happy', 'sad', or 'neutral'.\"}, # adjust for your use-case\n",
        "                {\"role\": \"user\", \"content\": f\"Here is the statement: `{test_case['input']}`\"} # adjust for your use-case\n",
        "            ],\n",
        "            logit_bias={\n",
        "                str(logit_bias_values['happy']): 100,\n",
        "                str(logit_bias_values['sad']): 100,\n",
        "                str(logit_bias_values['neutral']): 100\n",
        "            },\n",
        "            max_tokens=1,\n",
        "            temperature=0,\n",
        "        ).choices[0].message.content\n",
        "\n",
        "        end_time = time.time()\n",
        "        model_timers[model] += (end_time - start_time)\n",
        "\n",
        "        status = \"✅\" if x == test_case['expected_output'] else \"❌\"\n",
        "        row.append(status)\n",
        "\n",
        "        # Update model results\n",
        "        if x == test_case['expected_output']:\n",
        "            model_results[model]['correct'] += 1\n",
        "        model_results[model]['total'] += 1\n",
        "\n",
        "    table.add_row(row)\n",
        "\n",
        "print(table)\n",
        "\n",
        "# Calculate and print the percentage of correct answers and average time for each model\n",
        "for model in models:\n",
        "    correct = model_results[model]['correct']\n",
        "    total = model_results[model]['total']\n",
        "    percentage = (correct / total) * 100\n",
        "    avg_time = model_timers[model] / total\n",
        "    print(f\"{model} got {percentage:.2f}% correct. Average time: {avg_time:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "nouc6ZD4oIs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now that we've benchmarked, we can pull out the classifiers to use in external code.\n",
        "\n"
      ],
      "metadata": {
        "id": "1i-biO4AZ86v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, the 'true'/'false' classifier. Feel free to modify it for your use-case."
      ],
      "metadata": {
        "id": "ad6zkSdEaODF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.ChatCompletion.create(\n",
        "    model='gpt-3.5-turbo', # adjust to the ideal model for your use-case\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Determine if the input statement is true or false. Return 'true' if it is true, return 'false' if it is false.\"}, # adjust for your use-case\n",
        "        {\"role\": \"user\", \"content\": f\"Here is the statement: `{test_case['input']}`\"} # adjust for your use-case\n",
        "    ],\n",
        "    logit_bias={\n",
        "        '1904': 100,  # true to 100\n",
        "        '3934': 100,  # false to 100\n",
        "    },\n",
        "    max_tokens=1,\n",
        "    temperature=0,\n",
        ").choices[0].message.content"
      ],
      "metadata": {
        "id": "xL0FX0VBaNap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the dynamic classifier. Feel free to modify it for your use-case.\n",
        "\n",
        "Note -- to speed this up a bit, grab the outputs from `get_token_ids` and hardcode them into your OpenAI call."
      ],
      "metadata": {
        "id": "4UJAE8N-alKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get token IDs for a given model family\n",
        "def get_token_ids(model_family, text):\n",
        "    if 'gpt-4' in model_family:\n",
        "        model_enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "    else:\n",
        "        model_enc = tiktoken.encoding_for_model(\"gpt-3\")\n",
        "    return model_enc.encode(text)\n",
        "\n",
        "# Adjust these for your use-case\n",
        "logit_bias_values = {\n",
        "    'happy': get_token_ids('gpt-3.5-turbo', 'happy')[0],\n",
        "    'sad': get_token_ids('gpt-3.5-turbo', 'sad')[0],\n",
        "    'neutral': get_token_ids('gpt-3.5-turbo', 'neutral')[0]\n",
        "}\n",
        "\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "  model='gpt-3.5-turbo',\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"Determine the sentiment of the input statement. Return 'happy', 'sad', or 'neutral'.\"}, # adjust for your use-case\n",
        "      {\"role\": \"user\", \"content\": f\"Here is the statement: `{test_case['input']}`\"} # adjust for your use-case\n",
        "  ],\n",
        "  logit_bias={\n",
        "      str(logit_bias_values['happy']): 100,\n",
        "      str(logit_bias_values['sad']): 100,\n",
        "      str(logit_bias_values['neutral']): 100\n",
        "  },\n",
        "  max_tokens=1,\n",
        "  temperature=0,\n",
        ").choices[0].message.content"
      ],
      "metadata": {
        "id": "zPdlJwsAao2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}